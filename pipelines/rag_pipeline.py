import logging
from services.document_loader import crawl_website
from services.chromadb_service import index_documents
from pipelines.llm_interface import query_llm_pipeline
from services.redis_service import is_url_scraped, mark_url_as_scraped

def run_rag_pipeline(url: str, question: str) -> str:
    """
    Orchestrates the RAG pipeline: checks if the website has been processed,
    crawls and indexes if necessary, and queries the LLM.

    Args:
        url (str): The URL to scrape and index.
        question (str): The user's question to query the system.
        context_retriever: A retriever object for fetching relevant context.

    Returns:
        str: The answer generated by the LLM.
    """
    try:
        # Step 1: Check if the URL has already been processed
        if is_url_scraped(url):
            logging.info(f"URL '{url}' has already been scraped. Skipping crawling.")
        else:
            # Step 2: Crawl the website
            logging.info(f"Crawling website: {url}")
            documents = crawl_website(url)
            if not documents:
                raise ValueError(f"No documents found for URL: {url}")

            # Step 3: Index the documents into ChromaDB
            logging.info(f"Indexing {len(documents)} documents.")
            index_documents(documents)

            # Mark the URL as processed
            mark_url_as_scraped(url)

        # Step 4: Query the LLM using the retriever
        logging.info(f"Querying the LLM for the question: {question}")
        response = query_llm_pipeline(question)
        logging.info("RAG pipeline completed successfully.")
        return response
    except Exception as e:
        logging.error(f"Error in RAG pipeline: {e}", exc_info=True)
        raise
